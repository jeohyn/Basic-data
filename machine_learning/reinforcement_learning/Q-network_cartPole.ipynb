import gym
import numpy as np
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt

#stochastic
env=gym.make('CartPole-v0')

#to solve the problem of using placeholder and contrib in tensorflow 
tf.disable_v2_behavior()


#input and output size based on the Env.
#input=4(# of cases)
#ouput=all actions' reward=2
input_size=env.observation_space.shape[0]
output_size=env.action_space.n
learning_rate=1e-1

#establish the feed-forward part of the network used to choose actions
#X=2Dimensional array.[[4 values]]
X=tf.placeholder(tf.float32, [None, input_size], name="input_x") #state input. None==1.we will give one value at once
#X=2Dimensional array.
W1=tf.get_variable("W1", shape=[input_size, output_size], initializer=tf.contrib.layers.xavier_initializer()) 

#W*X. Q prediction. result : #X=2Dimensional array. [[2 values]]
Qpred=tf.matmul(X,W1)
#Y label. we need to define the parts of the network needed for learning a policy.
Y=tf.placeholder(shape=[None, output_size], dtype=tf.float32)

#because tf.square(Y-Qpred) is matrix, uses reduce_sum() to make one value
loss=tf.reduce_sum(tf.square(Y-Qpred))#cost function
#learning
train=tf.train.AdamOptimizer(learning_rate=learning_rate).minimize(loss)

#set parameters for Q-Network
dis=.9
num_episodes=5000

#create lists to conain total rewards and steps per episode
rList=[]

sess=tf.Session()
sess.run(tf.global_variables_initializer())

#learning
for i in range(num_episodes):
    #reset environment and get first new observation
    s=env.reset()
    e=1./((i/50)+10)
    rAll=0
    step_count=0
    done=False

    #Q-Network training
    while not done:
        step_count+=1
        #x=array [4 values]
        x=np.reshape(s,[1,input_size])
        #choose an action by greedily
        Qs=sess.run(Qpred, feed_dict={X:one_hot(s)})
        if np.random.rand(1)<e:
            a=env.action_space.sample()
        else:
            a=np.argmax(Qs)

        #get new state&reward from env
        s1, reward, done,_=env.step(a)
        #if pole fall
        if done:
            #update Q, and no Qs+1, since it's a terminal state
            #only update the action that we do->so Qs[0,a]
            Qs[0,a]=-100
        else:
            x1=np.reshape(s1,[1,input_size])
            #obtain the Q_s1 values by feeding the new state through our network
            Qs1=sess.run(Qpred, feed_dict={X:x1})
            #update Q
            Qs[0,a]=reward+dis*np.max(Qs1)

        #train network using target(Y) and predicted Q(Qpred) values
        sess.run(train, feed_dict={X:x, Y:Qs})
        s=s1
            
    rList.append(step_count)
    print("Episodes : {} steps : {}".format(i, step_count))

    #if lasat 10's avg steps are 500, it's good enough
    if len(rList)>10 and np.mean(rList[-10:])>500:
        break

#see our trained network in action
observation=env.reset()
reward_sum=0
while True:
    env.render()

    x=np.reshape(observation,[1,input_size])
    Qs=sess.run(Qpred, feed_dict={X:x})
    a=np.argmax(Qs)

    observation,reward,done,_=env.step(a)
    reward_sum+=reward

    if done:
        print("Total score: {}".format(reward_sum))
        break



