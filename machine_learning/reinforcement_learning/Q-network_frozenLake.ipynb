import gym
import numpy as np
import tensorflow.compat.v1 as tf
import matplotlib.pyplot as plt

#stochastic
env=gym.make('FrozenLake-v0')

#to solve the problem of using placeholder in tensorflow 
tf.disable_v2_behavior()


#use one-hot endoding_ state:np.identity(16)[s1:(s1+1)] (16 : # of moving cases)
def one_hot(x):
    return np.identity(16)[x:x+1]

#input and output size based on the Env.
#input=16(# of cases)
#ouput=all actions' reward=4
input_size=env.observation_space.n
output_size=env.action_space.n
learning_rate=0.1

#establish the feed-forward part of the network used to choose actions
#X=2Dimensional array.[[16 elements]]
X=tf.placeholder(shape=[1, input_size], dtype=tf.float32) #state input. input_size=16
#X=2Dimensional array.[[4 elements]*16]
W=tf.Variable(tf.random_uniform([input_size, output_size], 0, 0.01)) #weight. output_size=4

#W*X. Q prediction. result : #X=2Dimensional array.[[4 elements]]
Qpred=tf.matmul(X,W)
#Y label. we need to define the parts of the network needed for learning a policy.
Y=tf.placeholder(shape=[1, output_size], dtype=tf.float32)

#because tf.square(Y-Qpred) is matrix, uses reduce_sum() to make one value
loss=tf.reduce_sum(tf.square(Y-Qpred))#cost function
train=tf.train.GradientDescentOptimizer(learning_rate=learning_rate).minimize(loss)

#set parameters for Q-Network
dis=.99
num_episodes=2000

#create lists to conain total rewards and steps per episode
rList=[]

#learning
with tf.Session() as sess:
    sess.run(tf.global_variables_initializer())
    for i in range(num_episodes):
        #reset environment and get first new observation
        s=env.reset()
        e=1./((i/50)+10)
        rAll=0
        done=False
        local_loss=[]

        #Q-Network training
        while not done:
            #choose an action by greedily
            Qs=sess.run(Qpred, feed_dict={X:one_hot(s)})
            if np.random.rand(1)<e:
                a=env.action_space.sample()
            else:
                a=np.argmax(Qs)

            #get new state&reward from env
            s1, reward, done,_=env.step(a)
            if done:
                #update Q, and no Qs+1, since it's a terminal state
                #only update the action that we do->so Qs[0,a]
                Qs[0,a]=reward
            else:
                #obtain the Q_s1 values by feeding the new state through our network
                Qs1=sess.run(Qpred, feed_dict={X:one_hot(s1)})
                #update Q
                Qs[0,a]=reward+dis*np.max(Qs1)

            #train network using target(Y) and predicted Q(Qpred) values
            sess.run(train, feed_dict={X:one_hot(s), Y:Qs})

            rAll+=reward
            s=s1
        rList.append(rAll)

print("percentage of successful episodes:"+str(sum(rList)/num_episodes)+"%")
plt.bar(range(len(rList)), rList, color="blue")
plt.show()



